{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05eb3cde",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150> <br>\n",
    "<font color=0F5298 size=7>\n",
    "Artificial Intelligence <br>\n",
    "<font color=2565AE size=5>\n",
    "Computer Engineering Department <br>\n",
    "Spring 2024<br>\n",
    "<font color=3C99D size=5>\n",
    "Practical Assignment 2 - Minimax Algorithm<br>\n",
    "<font color=696880 size=4>\n",
    "Sepehr Harfi\n",
    "\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a011cf",
   "metadata": {},
   "source": [
    "# Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0d506013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your student number and name\n",
    "Student_number = '401105901'\n",
    "Name = 'Amirardalan'\n",
    "Last_name = 'Dehghanpour'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de0ad2",
   "metadata": {},
   "source": [
    "# Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf03f32",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Please run all the cells.\n",
    "\n",
    "Please refrain from making any changes to the existing files, such as `board.py`, `game.py`, etc. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f884c",
   "metadata": {},
   "source": [
    "#### **Note:** It is strongly recommended to execute this notebook on your local device instead of Google Colab due to limitations with graphics. However, if you still prefer using Google Colab, you can refer to this [link](https://vishnudsharma.medium.com/visualizing-tkinter-and-pygame-in-colab-272c5a245f8c) for assistance in using graphics with Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3fdd74",
   "metadata": {},
   "source": [
    "# Libraries & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94534a71",
   "metadata": {},
   "source": [
    "**Dont change the below cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5e4bb0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "curr_working_directory = os.getcwd()\n",
    "src_directory_path = os.path.join(curr_working_directory, \"src\")\n",
    "\n",
    "if os.path.exists(src_directory_path):\n",
    "    os.chdir(src_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "65bcdc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import game\n",
    "from player import Player\n",
    "from random_player import RandomPlayer\n",
    "from random_greedy_player import RandomGreedyPlayer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf190d9",
   "metadata": {
    "id": "5cf190d9"
   },
   "source": [
    "\n",
    "# Breakthrough Game: Minimax and AlphaBeta Players (100 Points)\n",
    "\n",
    "In this notebook, we will implement and compare two AI strategies, Minimax and AlphaBeta, for playing a simple version of the Breakthrough game. We aim to assess the performance of these strategies in terms of their execution time and win probability in matches against a Greedy player and against each other with varying depths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba059eaf",
   "metadata": {},
   "source": [
    "# Breakthrough Game Rules\n",
    "\n",
    "The Breakthrough game is a two-player strategy board game played on an 6x6 grid. The objective of the game is to move your pieces from one end of the board to the other before your opponent does.\n",
    "\n",
    "## Game Setup\n",
    "\n",
    "- The board consists of a 6x6 grid with alternating dark and light squares.\n",
    "- Each player starts with 12 pieces placed on their respective sides of the board.\n",
    "- The pieces are initially arranged in two rows, with each row containing 6 pieces.\n",
    "\n",
    "## Piece Movement\n",
    "\n",
    "- Each player can only move their own pieces.\n",
    "- Pieces can move diagonally or straight forward one square at a time.\n",
    "- Pieces cannot move backward or sideways.\n",
    "- Pieces can capture their opponent's pieces by moving diagonally forward and landing on a square occupied by an opponent's piece.\n",
    "- Captured pieces are removed from the board.\n",
    "\n",
    "\n",
    "## Game End\n",
    "\n",
    "- If a player successfully moves one of their pieces to the opposite end of the board, they win the game.\n",
    "\n",
    "## Additional Rules\n",
    "\n",
    "- Players can only capture the opponent's piece if its in their diagonal forward squares and they cannot have two or more pieces of their color in the same square.\n",
    "- Players take turns moving their pieces.\n",
    "- Players must make a move on their turn, and they cannot skip their turn.\n",
    "- If a player has multiple legal moves, they can choose which piece to move."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe681eb",
   "metadata": {
    "id": "ffe681eb"
   },
   "source": [
    "\n",
    "## Demo of the game with graphics\n",
    "\n",
    "Initially, using the cell below, you can see a demo gameplay of two Random Greedy Players to gain insights into the game. Additionally, you can explore the gameplay of other agents such as \"Player\" which plays in a complete greedy way and also \"RandomPlayer\", to further enhance your understanding of the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c41b4",
   "metadata": {},
   "source": [
    "**Note:** You can use the cell below anywhere you want to see the game with the graphics. For this purpose, you should only be careful to pass the right class of players to the `game.Game()`. Also, If you want to see the performance of your Minimax or AlphaBeta agents, you may need to add their classes as a python file to the `src` folder and import their classes in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4b067d36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b067d36",
    "is_executing": true,
    "outputId": "c31c275e-abbe-4383-c984-80922eb2d829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: -192.0, 1: 213.0}\n"
     ]
    }
   ],
   "source": [
    "%%python3 \n",
    "# Change the above line to %%python if youre using Windows OS\n",
    "import game\n",
    "from player import Player\n",
    "from random_greedy_player import RandomGreedyPlayer\n",
    "from random_player import RandomPlayer\n",
    "from minimax_player import MinimaxPlayer\n",
    "from alphabeta_player import AlphaBetaPlayer\n",
    "\n",
    "\n",
    "random_greedy_game = game.Game(MinimaxPlayer,AlphaBetaPlayer)\n",
    "print(random_greedy_game.play(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac79cb",
   "metadata": {
    "id": "d6ac79cb"
   },
   "source": [
    "\n",
    "## Minimax Agent (30 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee44bc1",
   "metadata": {},
   "source": [
    "To gain insights into the game, start by reading the `board.py`, `game.py`, and `player.py` files to understand the game implementation. Then, implement a minimax agent. Recall that the minimax algorithm evaluates game states and selects optimal moves. Compare the performance of the random greedy and minimax agents to gain insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c64f4",
   "metadata": {},
   "source": [
    "**Note:** To implement the Minimax agent, you should only modify the `MinimaxPlayer` class in the cell below. You can either use the Board class `get_scores` function or define your own score funcion here and use it for the evaluation of a board state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd51e9",
   "metadata": {},
   "source": [
    "**Note:** Your minimax implementation should have a depth instance which determines the level to which the algorithm should be executed for each move. It controls the extent of the search tree explored by the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063de25",
   "metadata": {},
   "source": [
    "**Note:** Feel free to add cells if you need to, but don't erase the existing cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf52f5b",
   "metadata": {},
   "source": [
    "### Implementation (25 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7f1a446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from player import Player\n",
    "import numpy as np\n",
    "class MinimaxPlayer(Player):\n",
    "    def __init__(self, player_number, board, depth=3):\n",
    "        super().__init__(player_number, board)\n",
    "        self.depth = depth\n",
    "        self.limit=0\n",
    "\n",
    "    def get_next_move(self):\n",
    "      #  self.minimax(0,self.player_number)\n",
    "      #  self.board.start_imagination()\n",
    "        # default is greedy playing\n",
    "        max_score = -float('inf')\n",
    "        best_move = None\n",
    "        range_n = 0, self.board.get_n()\n",
    "        step = 1\n",
    "        if self.player_number == 0:\n",
    "            range_n = self.board.get_n() - 1, -1\n",
    "            step = -1\n",
    "        moves=self.get_moves(range_n[0],range_n[1],step,self.player_number,self.board.get_board_grid(),self.board)\n",
    "        for move in moves:\n",
    "                    self.board.start_imagination()\n",
    "                    if self.board.is_move_valid(self.board.get_imaginary_board(), move, self.player_number):\n",
    "                        scores, game_over = self.board.place_piece_imaginary(move, self.player_number)\n",
    "                        score=self.Minimax(self.depth,not self.player_number)\n",
    "                        if score > max_score:\n",
    "                            max_score=score\n",
    "                            best_move = move\n",
    "        return best_move\n",
    "    def Minimax(self,depth,player_number):\n",
    "        if(depth==0 or self.board.is_game_over(self.board.get_imaginary_board())):\n",
    "             return (self.board.get_scores(self.board.get_imaginary_board())[self.player_number] -\n",
    "                     self.board.get_scores(self.board.get_imaginary_board())[self.opponent_number])\n",
    "        range_n = 0, self.board.get_n()\n",
    "        step = 1\n",
    "        if player_number == 0:\n",
    "            range_n = self.board.get_n() - 1, -1\n",
    "            step = -1\n",
    "            max_score=-float('inf')\n",
    "        if player_number:\n",
    "            max_score=float('inf')\n",
    "        if player_number==self.player_number:\n",
    "            moves=self.get_moves(range_n[0],range_n[1],step,player_number,self.board.get_imaginary_board(),self.board)\n",
    "            for move in moves:\n",
    "                    save_board=np.copy(self.board.get_imaginary_board())\n",
    "                    if self.board.is_move_valid(self.board.get_imaginary_board(), move,player_number):\n",
    "                        self.board.place_piece_imaginary(move,player_number)\n",
    "                    score=self.Minimax(depth-1,not player_number)\n",
    "                    score=max(score,max_score)\n",
    "                    self.board.imaginary_board_grid=np.copy(save_board)\n",
    "            return max_score\n",
    "        else:\n",
    "            moves=self.get_moves(range_n[0],range_n[1],step,player_number,self.board.get_imaginary_board(),self.board)\n",
    "            for move in moves:\n",
    "                    save_board=np.copy(self.board.get_imaginary_board())\n",
    "                    if self.board.is_move_valid(self.board.get_imaginary_board(), move,player_number):\n",
    "                        self.board.place_piece_imaginary(move,player_number)\n",
    "                    score= self.Minimax(depth-1,not player_number)\n",
    "                    score=min(score,max_score)\n",
    "                    self.board.imaginary_board_grid=np.copy(save_board)\n",
    "            return max_score\n",
    "    \n",
    "        # TODO: Implement this function based on the minimax algorithm\n",
    "        pass\n",
    "\n",
    "    def get_moves(self,range_1,range_2,step,player_number,test_board,board):\n",
    "        moves=[]\n",
    "        for i in range(range_1,range_2,step):\n",
    "            for j in range(range_1,range_2,step):\n",
    "                if test_board[i][j] == player_number:\n",
    "                 for direction in board.get_possible_directions(player_number):\n",
    "                    move = (i, j), (i + direction[0], j + direction[1])\n",
    "                    moves.append(move)\n",
    "        return moves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb6b4c",
   "metadata": {},
   "source": [
    "### Time Analysis (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c148af",
   "metadata": {},
   "source": [
    "Now play the game 5 times between two random players and calculate the average execution time. Also do this for the game of a random player and a minimax player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c27ce0cf",
   "metadata": {
    "id": "c27ce0cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Players Average Execution Time: 0.0030556201934814455 seconds\n",
      "Minimax Player and Random Player Average Execution Time: 1.4060933113098144 seconds\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate and print the average execution time of two random players\n",
    "import time\n",
    "sum=0\n",
    "for i in range(0,5):\n",
    "    start = time.time()\n",
    "    random_game = game.Game(RandomPlayer,RandomPlayer)\n",
    "    random_game.play(False)\n",
    "    stop = time.time()\n",
    "    sum=sum+(stop-start)  \n",
    "\n",
    "print(\"Random Players Average Execution Time:\", sum/5, \"seconds\")\n",
    "\n",
    "# TODO: calculate and print the average execution time of a random player and a minimax players with depth=3 \n",
    "sum=0\n",
    "for i in range(0,5):\n",
    "    start = time.time()\n",
    "    minimax_game = game.Game(MinimaxPlayer,RandomPlayer)\n",
    "    minimax_game.play(False)\n",
    "    stop = time.time()\n",
    "    sum=sum+(stop-start)  \n",
    "\n",
    "print(\"Minimax Player and Random Player Average Execution Time:\", sum/5, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46ed00",
   "metadata": {},
   "source": [
    "## Alphabeta Agent (30 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b1458",
   "metadata": {},
   "source": [
    "As we can see from the above code, the Minimax algorithm takes much longer time to execute. To improve the execution time, we can implement the AlphaBeta pruning algorithm. In the next cell, we will be implementing the AlphaBeta player.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa687f",
   "metadata": {},
   "source": [
    "### Implementation (25 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0e20ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from player import Player\n",
    "import numpy as np\n",
    "class AlphaBetaPlayer(Player):\n",
    "    def __init__(self, player_number, board, depth=3):\n",
    "        super().__init__(player_number, board)\n",
    "        self.depth = depth\n",
    "        self.limit=0\n",
    "\n",
    "    def get_next_move(self):\n",
    "      #  self.minimax(0,self.player_number)\n",
    "      #  self.board.start_imagination()\n",
    "        # default is greedy playing\n",
    "        alpha=-float('inf')\n",
    "        beta=float('inf')\n",
    "        max_score = -float('inf')\n",
    "        best_move = None\n",
    "        range_n = 0, self.board.get_n()\n",
    "        step = 1\n",
    "        if self.player_number == 0:\n",
    "            range_n = self.board.get_n() - 1, -1\n",
    "            step = -1\n",
    "        moves=self.get_moves(range_n[0],range_n[1],step,self.player_number,self.board.get_board_grid(),self.board)\n",
    "        for move in moves:\n",
    "                    self.board.start_imagination()\n",
    "                    if self.board.is_move_valid(self.board.get_imaginary_board(), move, self.player_number):\n",
    "                        scores, game_over = self.board.place_piece_imaginary(move, self.player_number)\n",
    "                        score=self.alphabeta_Minimax(self.depth,not self.player_number,alpha,beta)\n",
    "                        if score > max_score:\n",
    "                            max_score=score\n",
    "                            best_move = move\n",
    "        return best_move\n",
    "    def alphabeta_Minimax(self,depth,player_number,alpha,beta):\n",
    "        if(depth==0 or self.board.is_game_over(self.board.get_imaginary_board())):\n",
    "             return (self.board.get_scores(self.board.get_imaginary_board())[self.player_number] -\n",
    "                     self.board.get_scores(self.board.get_imaginary_board())[self.opponent_number])\n",
    "        range_n = 0, self.board.get_n()\n",
    "        step = 1\n",
    "        if player_number == 0:\n",
    "            range_n = self.board.get_n() - 1, -1\n",
    "            step = -1\n",
    "            maxEva=-float('inf')\n",
    "        if player_number:\n",
    "            maxEva=float('inf')\n",
    "        if player_number==self.player_number:\n",
    "            moves=self.get_moves(range_n[0],range_n[1],step,player_number,self.board.get_imaginary_board(),self.board)\n",
    "            for move in moves:\n",
    "                    save_board=np.copy(self.board.get_imaginary_board())\n",
    "                    if self.board.is_move_valid(self.board.get_imaginary_board(), move,player_number):\n",
    "                        self.board.place_piece_imaginary(move,player_number)\n",
    "                    eva=self.alphabeta_Minimax(depth-1,not player_number,alpha,beta)\n",
    "                    maxEva=max(maxEva,eva)\n",
    "                    alpha=max(alpha,maxEva)\n",
    "                    if beta<=alpha:\n",
    "                         break\n",
    "                    self.board.imaginary_board_grid=np.copy(save_board)\n",
    "            return maxEva\n",
    "        else:\n",
    "            moves=self.get_moves(range_n[0],range_n[1],step,player_number,self.board.get_imaginary_board(),self.board)\n",
    "            for move in moves:\n",
    "                    save_board=np.copy(self.board.get_imaginary_board())\n",
    "                    if self.board.is_move_valid(self.board.get_imaginary_board(), move,player_number):\n",
    "                        self.board.place_piece_imaginary(move,player_number)\n",
    "                    eva= self.alphabeta_Minimax(depth-1,not player_number,alpha,beta)\n",
    "                    maxEva=min(maxEva,eva)\n",
    "                    beta=min(beta,eva)\n",
    "                    if beta<=alpha:\n",
    "                         break\n",
    "                    self.board.imaginary_board_grid=np.copy(save_board)\n",
    "            return maxEva\n",
    "    \n",
    "        # TODO: Implement this function based on the minimax algorithm\n",
    "        pass\n",
    "\n",
    "    def get_moves(self,range_1,range_2,step,player_number,test_board,board):\n",
    "        moves=[]\n",
    "        for i in range(range_1,range_2,step):\n",
    "            for j in range(range_1,range_2,step):\n",
    "                if test_board[i][j] == player_number:\n",
    "                 for direction in board.get_possible_directions(player_number):\n",
    "                    move = (i, j), (i + direction[0], j + direction[1])\n",
    "                    moves.append(move)\n",
    "        return moves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c703a67f",
   "metadata": {},
   "source": [
    "### Time Analysis (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83174e",
   "metadata": {},
   "source": [
    "Now play the game 5 times between the random player and an alphabeta player and calculate the average execution time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bce1a47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabeta Player and Random Player Average Execution Time: 0.18294792175292968 seconds\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate and print the average execution time of a random player and an alpha-beta players with depth=3 \n",
    "sum=0\n",
    "for i in range(0,5):\n",
    "    start = time.time()\n",
    "    alphabeta_game = game.Game(AlphaBetaPlayer,RandomPlayer)\n",
    "    alphabeta_game.play(False)\n",
    "    stop = time.time()\n",
    "    sum=sum+(stop-start)   \n",
    "print(\"Alphabeta Player and Random Player Average Execution Time:\", sum/5, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480857e5",
   "metadata": {},
   "source": [
    "##### **Note:** The alphabeta agent should be approximately 5X-10X faster than the minimax player."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6770103",
   "metadata": {
    "id": "c6770103"
   },
   "source": [
    "\n",
    "## Win Probability Analysis (35 Points)\n",
    "\n",
    "In this section, We will simulate 50 games for your AlphaBeta player against other players and analyze their win rates. Additionally, we will have AlphaBeta players with different depths play against each other. \n",
    "\n",
    "Assume you are always the second player and the black player will always start first. \n",
    "\n",
    "If the agent is implemented correctly, with a depth of two or more it should win all the mentioned agents with a win rate > 0.7.\n",
    "\n",
    "**Note:** You can use the `bulk_play` function from `game.py` for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fb476097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "depths = [1, 2, 3]  # List of different second_player_depth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0e54af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results_different_depths(first_player_class, depths, first_player_depth=None, n=50):\n",
    "    results = []\n",
    "    sample_game = game.Game(first_player_class, AlphaBetaPlayer)\n",
    "\n",
    "    for depth in depths:\n",
    "        if first_player_depth is not None:\n",
    "            result = sample_game.bulk_play(n, first_player_depth=first_player_depth, second_player_depth=depth)\n",
    "        else:\n",
    "            result = sample_game.bulk_play(n, second_player_depth=depth)\n",
    "        win_rate = result['white'] / n  # Calculate win rate\n",
    "        results.append({'Depth': depth, 'Win Rate': win_rate})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5361f9",
   "metadata": {},
   "source": [
    "### Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e8139c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Depth  Win Rate\n",
      "0      1       1.0\n",
      "1      2       1.0\n",
      "2      3       1.0\n"
     ]
    }
   ],
   "source": [
    "results = compute_results_different_depths(RandomPlayer, depths)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ea0e7f",
   "metadata": {},
   "source": [
    "### Random Greedy Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "27e22180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Depth  Win Rate\n",
      "0      1       1.0\n",
      "1      2       1.0\n",
      "2      3       1.0\n"
     ]
    }
   ],
   "source": [
    "results = compute_results_different_depths(RandomGreedyPlayer, depths)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e46530",
   "metadata": {},
   "source": [
    "### Greedy Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "37408be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Depth  Win Rate\n",
      "0      1       1.0\n",
      "1      2       1.0\n",
      "2      3       1.0\n"
     ]
    }
   ],
   "source": [
    "results = compute_results_different_depths(Player, depths)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3218a4f",
   "metadata": {},
   "source": [
    "### Alphabeta Player with depth 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c7374",
   "metadata": {},
   "source": [
    "**Note:** In this part each game may take up to ~45 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3cd374cc",
   "metadata": {
    "id": "3cd374cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Depth  Win Rate\n",
      "0      2       1.0\n",
      "1      3       1.0\n",
      "2      4       1.0\n"
     ]
    }
   ],
   "source": [
    "results = compute_results_different_depths(AlphaBetaPlayer, [2, 3, 4], first_player_depth=2, n=30)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873b294",
   "metadata": {
    "id": "4873b294"
   },
   "source": [
    "\n",
    "## Conclusion (5 Points)\n",
    "\n",
    "#### Based on the results of the AlphaBeta player with other players, what is your conclusion?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7783dc",
   "metadata": {},
   "source": [
    "**Answer:**We concluse from games that best choice for algorithm is AlphaBeta player beacuse it's much faster than minimax algorithm and also have best choice same as minimax,we saw that alphabeta can reduce runtime of alogrithm about ten times,but after that minimax player can win game another players such as greedy or random. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
